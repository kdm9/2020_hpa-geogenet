---
title: Construct on Hpa
date: 2021-01-02
author: Kevin Murray
---

Have now separated out the conStruct notebook.

```{r setup, include=F, cache=F}
if (!require("tidyverse"))    { install.packages("tidyverse")     ; require("tidyverse")    }
if (!require("foreach"))      { install.packages("foreach")       ; require("foreach")      }
if (!require("doParallel"))   { install.packages("doParallel")    ; require("doParallel")   }
if (!require("parallel"))     { install.packages("parallel")      ; require("parallel")     }
if (!require("ggplot2"))      { install.packages("ggplot2")       ; require("ggplot2")      }
if (!require("ggrepel"))      { install.packages("ggrepel")       ; require("ggrepel")      }
if (!require("ggmap"))        { install.packages("ggmap")         ; require("ggmap")        }
if (!require("RColorBrewer")) { install.packages("RColorBrewer")  ; require("RColorBrewer") }
if (!require("fossil"))       { install.packages("fossil")        ; require("fossil")       }
if (!require("vegan"))        { install.packages("vegan")         ; require("vegan")        }
if (!require("conStruct"))    { install.packages("conStruct")     ; require("conStruct")    }
if (!require("SNPRelate"))    { BiocManager::install("SNPRelate") ; require("SNPRelate")    }

if (file.exists("data/cache/01_hpa-geogenetics.Rda")) load(file="data/cache/01_hpa-geogenetics.Rda")

NCPUS = as.integer(Sys.getenv("NCPUS", parallel::detectCores(logical=F)))
registerDoParallel(cores=NCPUS)
cat(paste("Using", NCPUS, "cores\n"))
```

## Geographic clustering

First, we need to select samples and find geographic clusters.

```{r}
geo.dist.indiv = eu.geo %>%
    column_to_rownames(var = "ind") %>%
    select(longitude, latitude) %>%
    fossil::earth.dist()
geo.clust.indiv = geo.dist.indiv %>%
    hclust() %>%
    cutree(h=5) # cut at 5km radius
geo.cluster.name = sprintf("GC%02d", geo.clust.indiv)
eu.geo$geo.clust = geo.cluster.name
```

So at a population radius of 5km, we have `r length(geo.cluster.name)`
clusters.

Here are the cluster sizes (y clusters of size x): 

```{r}
table(table(geo.cluster.name))
```


## Basic genetics

Here we need to extract the SNP data, and reduce it to allele frequencies at
each geographic cluster, and remove snps with null variance.


First,  get the indivs by snps matrix (122x500k)

```{r gdsopen}
gds = snpgdsOpen("data/HaR.filtered_snps_final.PASS.bi.hardFiltered.indFiltered.noMit.reheader.gds")
gds.sum  = snpgdsSummary(gds)
gen = snpgdsGetGeno(gds, sample.id = samp.within.eur, with.id = T)
```

Now, we reduce the 122x500k matrix of SNPs to a 68x500k matrix of population
allele frequencies. We skip columns (SNPs) with null variance. 

```{r}
gen.pop = xfun::cache_rds({
    N.SNP = ncol(gen$genotype)
    foreach(i=seq_len(N.SNP), .combine=cbind) %do% {
        v = var(gen$genotype[, i], na.rm=T)
        if (!is.finite(v) || v <=0) {
            return(NULL)
        }
        tapply(gen$genotype[, i], geo.cluster.name, function(x) {mean(x, na.rm=T)/2})
    }
}, file="02_01_genpop", dir="data/cache/",  compress="xz")
```



## Run construct

Here we do some data prep, then run all the construct runs needed to do cross
validation (NB: we don't use construct's own cross validation here as it runs
all K sequentially which makes it take K times as long as it needs to.

```{r}
geo.clust.dat = eu.geo %>%
    group_by(geo.clust, pop) %>%
    summarise(latitude = mean(latitude),
              longitude = mean(longitude)) %>%
    ungroup()
geo.clust.latlong = geo.clust.dat %>%
    select(longitude, latitude) %>% 
    as.matrix()
geo.clust.dist = geo.clust.latlong %>%
    fossil::earth.dist() %>%
    as.matrix()
```

So to run construct we shell out to the cluster, as it takes ages. So, we save
the inputs to an Rds and then load the results again here for plotting and
interpretation.

```{r}
kdm.x.validation <- function(train.prop = 0.9, n.reps, K, freqs = NULL, data.partitions = NULL, geoDist, coords, prefix, n.iter, make.figs = FALSE, save.files = FALSE, n.nodes=16, parallel=T, ...) {
    call.check <- conStruct:::check.xval.call(args <- as.list(environment()))
    data.partitions <- conStruct:::make.data.partitions(n.reps,freqs,train.prop)
    conStruct:::check.data.partitions.arg(args <- as.list(environment()))
    cat("Data partitions created successfully\n")
    n.mdl = 2 * n.reps * length(K)
    x.val = foreach::foreach(rep.no=1:n.reps, .combine=bind_rows) %:%
        foreach::foreach(k=K, .combine=bind_rows) %:%
        foreach::foreach(mdl=c("sp", "nsp"), .combine=bind_rows) %dopar% {
            spatial = mdl == "sp"
            cs = conStruct:::xval.conStruct(spatial = spatial, K = k, 
                            data = data.partitions[[rep.no]]$training, 
                            geoDist = geoDist, coords = coords, 
                            prefix = paste0(prefix, "_", mdl, "_", "rep", rep.no, "K", k), 
                            n.iter = n.iter, make.figs = make.figs, save.files = save.files)
            cat(paste0("mdl=", mdl, " K=",k, " rep=", rep.no, " Done\n"))
            list(mdl=mdl, K=k, rep=rep.no, construct.res=cs, data.part=data.partitions[[rep.no]])
    }
    save(data.partitions,file=paste0(prefix, ".xval.data.partitions.Robj"))
    save(x.val,file=paste0(prefix, ".xval.results.Robj"))
    x.val
}
```

```{r}
cxv.tmp = xfun::cache_rds({
    kdm.x.validation(
        prefix="out/Hpa_cs/HpA_run_tmp",
        freqs=gen.pop,
        coords=geo.clust.latlong,
        geoDist=geo.clust.dist,
        K=1:2,
        train.prop=0.9,
        n.reps=2,
        n.iter=1000,
        n.nodes=NULL,
        save.files=T,
        make.figs=T)
}, file="02_02_cs_xval_tmp", dir="data/cache/",  compress="xz")
```


```{r run.xval}
if (!dir.exists("out/Hpa_cs/")) dir.create("out/Hpa_cs")
cxv = xfun::cache_rds({
    kdm.x.validation(
        prefix="out/Hpa_cs/HpA_run2",
        freqs=gen.pop,
        coords=geo.clust.latlong,
        geoDist=geo.clust.dist,
        K=1:6,
        train.prop=0.9,
        n.reps=8,
        n.iter=20000,
        n.nodes=NULL,
        save.files=T,
        make.figs=T)
}, file="02_02_cs_xval", dir="data/cache/",  compress="xz")
```

# Summarise construct cross validation

first, reshape the nested lists to a tibble

```{r}
str(cxv, max.level=4)
cxv2 = unlist(cxv, recursive=F) %>%
    unlist(recursive=F) %>%
    do.call(bind_rows, .)
cxv2
```

```{r}
conStruct:::fit.to.test(

```


# Finalise

```{r cleanup, include=F, cache=F}
save.image(file="data/cache/02_construct.Rda", compress="xz")
```
